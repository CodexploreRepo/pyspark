{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Skew\n",
    "<p align='center'><img src='.././images/data_skew.webp' height=300><br>Figure: even (left) vs. uneven (right) data skew. </p>\n",
    "\n",
    "Reference: \n",
    "- [Solving Data Skewness in Spark\n",
    "](https://www.junaideffendi.com/blog/solving-data-skewness-in-spark/)\n",
    "- [Apache Spark Core – Practical Optimization Daniel Tomes (Databricks)\n",
    "](https://www.youtube.com/watch?v=_ArCesElWp8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"data-skew\")\\\n",
    "        .config(\"spark.ui.port\", \"4050\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.10.135:4051\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>data-skew</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd4f7f83370>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"true\")\\\n",
    "               .option(\"nullValue\", \"?\") \\\n",
    "               .option(\"inferSchema\", \"true\")\\\n",
    "               .csv(\"../data/linkage/block_*.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Shuffle Partition\n",
    "- Based on your dataset size, number of cores, and memory, Spark shuffling can benefit or harm your jobs. When you dealing with less amount of data, you should typically reduce the shuffle partitions otherwise you will end up with many partitioned files with a fewer number of records in each partition. which results in running many tasks with lesser data to process.\n",
    "\n",
    "- On another hand, when you have too much data and have less number of partitions results in fewer longer running tasks, and sometimes you may also get out of memory error.\n",
    "\n",
    "- Getting the right size of the shuffle partition is always tricky and takes many runs with different values to achieve the optimized number. This is one of the key properties to look for when you have performance issues on Spark jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\") #default 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5749132"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------\n",
      " id_1         | 15879             \n",
      " id_2         | 77381             \n",
      " cmp_fname_c1 | 1.0               \n",
      " cmp_fname_c2 | null              \n",
      " cmp_lname_c1 | 0.142857142857143 \n",
      " cmp_lname_c2 | null              \n",
      " cmp_sex      | 1                 \n",
      " cmp_bd       | 1                 \n",
      " cmp_bm       | 0                 \n",
      " cmp_by       | 0                 \n",
      " cmp_plz      | 0                 \n",
      " is_match     | false             \n",
      "-RECORD 1-------------------------\n",
      " id_1         | 22735             \n",
      " id_2         | 51381             \n",
      " cmp_fname_c1 | 1.0               \n",
      " cmp_fname_c2 | null              \n",
      " cmp_lname_c1 | 0.0               \n",
      " cmp_lname_c2 | null              \n",
      " cmp_sex      | 1                 \n",
      " cmp_bd       | 0                 \n",
      " cmp_bm       | 0                 \n",
      " cmp_by       | 1                 \n",
      " cmp_plz      | 0                 \n",
      " is_match     | false             \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.repartition(5, 'cmp_sex')\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|partition|  count|\n",
      "+---------+-------+\n",
      "|        1| 258703|\n",
      "|        3|5490429|\n",
      "+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# to get the counts of rows per partition.\n",
    "df.withColumn(\"partition\", F.spark_partition_id()).groupBy(\"partition\").count().show() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Skew Correction ?\n",
    "- In the real world, perfect data distributions are rare. Often when reading data, we are pulling from pre-partitioned files or ETL pipelines which may not automatically be distributed as nicely."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Repartition by Column(s)\n",
    "- The first solution is to logically re-partition your data based on the transformations\n",
    "    -  If you’re grouping or joining, partitioning by the groupBy/join columns can improve shuffle efficiency.\n",
    "\n",
    "`df = df.repartition(<n_partitions>, '<col_1>', '<col_2>',...)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Salt\n",
    "- SALTING is a common technique to solve data skews. \n",
    "- The idea is to add a random key to distribute data evenly between join keys\n",
    "- create a column with a random value the partition by that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:===========>                                              (1 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|SPARK_PARTITION_ID()| count|\n",
      "+--------------------+------+\n",
      "|                   0|717940|\n",
      "|                   1|717987|\n",
      "|                   2|718592|\n",
      "|                   3|720004|\n",
      "|                   4|719440|\n",
      "|                   5|717955|\n",
      "|                   6|718421|\n",
      "|                   7|718793|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('salt', F.rand())\n",
    "df = df.repartition(8, 'salt')\n",
    "df.groupBy(F.spark_partition_id()).count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
