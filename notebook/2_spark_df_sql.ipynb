{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Preparation for Notebook\n",
    "```Shell\n",
    "$ mkdir linkage\n",
    "$ cd linkage/\n",
    "$ curl -L -o donation.zip https://bit.ly/1Aoywaq\n",
    "$ unzip donation.zip\n",
    "$ unzip 'block_*.zip'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/19 19:22:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Start Spark Session: since this is local, so only can start 1 cluster\n",
    "spark=SparkSession.builder.appName('donation').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data IO\n",
    "- reading and writing dataframes in a variety of formats via the `DataFrameReader` and `DataFrameWriter` APIs\n",
    "    - **parquet**: Leading columnar-oriented data storage format (default option in Spark)\n",
    "    - **orc**: Another columnar-oriented data storage format\n",
    "    - **json**: Supports many of the same schema-inference functionality that the CSV format does\n",
    "    - **jdbc**: Connects to a relational database via the JDBC data connection standard\n",
    "    - **avro**: Provides efficient message serialization and deserialization when using a streaming source such as Apache Kafka\n",
    "    - **text**: Maps each line of a file to a dataframe with a single column of type string\n",
    "    - **image**: Loads image files from a directory as a dataframe with one column, containing image data stored as image schema\n",
    "    - **libsvm**: Popular text file format for representing labeled observations with sparse features\n",
    "    - **binary**: Reads binary files and converts each file into a single dataframe row (new in Spark 3.0)\n",
    "    - **xml**: Simple text-based format for representing structured information such as documents, data, configuration, or books (available via the spark-xml package)\n",
    "\n",
    "#### 1.1. Load Data \n",
    "- To read data, you access **DataFrameReader** API by calling the `read` method on a SparkSession instance\n",
    "```Python\n",
    "# Method 1: format() & load()\n",
    "d1 = spark.read.format(\"json\").load(\"file.json\")\n",
    "# Method 2: json(), csv()\n",
    "d2 = spark.read.json(\"file.json\")\n",
    "```\n",
    "\n",
    "#### 1.2. Write Data\n",
    "- To write data out again, you access the **DataFrameWriter** API via the `write`\n",
    "```Python\n",
    "d1.write.format(\"parquet\").save(\"file.parquet\")\n",
    "d1.write.parquet(\"file.parquet\")\n",
    "```\n",
    "- By default, Spark will throw an error if you try to save a dataframe to a file that already exists.\n",
    "    - You can control Spark’s behavior in this situation via the `mode`\n",
    "        - `Overwrite` the existing file, \n",
    "        - `Append` the data in the DataFrame to the file (if it exists), or \n",
    "        - `Ignore` the write operation if the file already exists and leave it in place\n",
    "\n",
    "```Python\n",
    "d2.write.format(\"parquet\").mode(\"overwrite\").save(\"file.parquet\")\n",
    "```\n",
    "\n",
    "#### 1.3. Select\n",
    "```Python\n",
    "df.select(\"col_1\", \"col_2\")\n",
    "```\n",
    "\n",
    "#### 1.4. `option`\n",
    "- Each of the different file formats has its own set of options that can be set \n",
    "    - Like for CSV, `.option(\"nullValue\", \"?\")`: replacing \"?\" with nullValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parsed = spark.read.option(\"header\", \"true\").option(\"nullValue\", \"?\").\\\n",
    "          option(\"inferSchema\", \"true\").csv(\"../data/linkage/block_*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_1: integer (nullable = true)\n",
      " |-- id_2: integer (nullable = true)\n",
      " |-- cmp_fname_c1: double (nullable = true)\n",
      " |-- cmp_fname_c2: double (nullable = true)\n",
      " |-- cmp_lname_c1: double (nullable = true)\n",
      " |-- cmp_lname_c2: double (nullable = true)\n",
      " |-- cmp_sex: integer (nullable = true)\n",
      " |-- cmp_bd: integer (nullable = true)\n",
      " |-- cmp_bm: integer (nullable = true)\n",
      " |-- cmp_by: integer (nullable = true)\n",
      " |-- cmp_plz: integer (nullable = true)\n",
      " |-- is_match: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To see the inferred type for each column\n",
    "parsed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Defining a schema using `StructType` and `StructField`\n",
    "- If you know the schema that you want to use for a file ahead of time, you can \n",
    "    - Method 1: Create an instance of the `pyspark.sql.types.StructType` class, named as `schema` and pass it to the Reader API via the schema function.\n",
    "    - Method 2: Another way to define the `schema` is using **DDL (data definition language)** statements: `\"id_1 INT, id_2 INT, cmp_fname_c1 DOUBLE\"`\n",
    "- Why ?: This can have a significant performance benefit when the dataset is very large, since Spark will not need to perform an extra pass over the data to figure out the data type of each column.\n",
    "```Python\n",
    "park.read.schema(schema).csv(\"...\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "# Method 1: define schema via StructType Class\n",
    "schema = StructType([StructField(\"id_1\", IntegerType(), False),\n",
    "  StructField(\"id_2\", StringType(), False),\n",
    "  StructField(\"cmp_fname_c1\", DoubleType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id_1', IntegerType(), False), StructField('id_2', StringType(), False), StructField('cmp_fname_c1', DoubleType(), False)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyzing Data with the DataFrame API\n",
    "#### Spark Transformation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Spark Action\n",
    "Cause Spark to execute recipe to transform source\n",
    "- `show(n)`, `first()`: prints the first n rows of the DataFrame\n",
    "- `take(n)`: returns the first n rows as a list of Row\n",
    "- `collect()`: return all the records as a list of Row \n",
    "    - **WARNING**: make sure will fit in driver program, especially for extremely large DataFrames, using these methods can be *dangerous and cause an out-of-memory* exception\n",
    "    -  `toPandas` (same as `collect`) method to return all the contents of a DataFrame to the client as an `Array`. \n",
    "- `count`:  returns the number of objects in an DataFrame   \n",
    "- `describe(*col)`:  computes statistics (count, mean, stddev, min, max) for numeric columns\n",
    "    -  if no columns are given, this function computes statistics for all numerical columns\n",
    "#### Handling `null` value:\n",
    "- `df.fillna(0, subset=good_features)`: to fill null with 0, and only on certain columns, in this example is a list of good_features columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| 3148| 8326|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|14055|94934|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|33948|34740|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|  946|71870|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|64880|71676|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id_1=3148, id_2=8326, cmp_fname_c1=1.0, cmp_fname_c2=None, cmp_lname_c1=1.0, cmp_lname_c2=None, cmp_sex=1, cmp_bd=1, cmp_bm=1, cmp_by=1, cmp_plz=1, is_match=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.first() #first element of the DataFrame, useful for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5749132"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Cache\n",
    "- `cache()` After the data has been parsed once, we’d like to save the data in its parsed form on the cluster so that we don’t have to reparse it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_1: int, id_2: int, cmp_fname_c1: double, cmp_fname_c2: double, cmp_lname_c1: double, cmp_lname_c2: double, cmp_sex: int, cmp_bd: int, cmp_bm: int, cmp_by: int, cmp_plz: int, is_match: boolean]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|is_match|  count|\n",
      "+--------+-------+\n",
      "|    true|  20931|\n",
      "|   false|5728201|\n",
      "+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parsed.groupBy('is_match').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Refer Cols\n",
    "- There are 2 ways we can reference the names of the columns in the DataFrame: \n",
    "    - literal strings, like in groupBy(\"is_match\"), \n",
    "    - `col` objects from `pyspark.sql.functions` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|is_match|  count|\n",
      "+--------+-------+\n",
      "|   false|5728201|\n",
      "|    true|  20931|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# col: return ~pyspark.sql.Column based on the given column name.\n",
    "#      in this case, we want to order by column count of the result\n",
    "parsed.groupBy('is_match').count().orderBy(col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Dataframe aggregation functions\n",
    "-  using the `agg` method of the **DataFrame API** in conjunction with the aggregation functions defined in the `pyspark.sql.functions collection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|     avg(cmp_sex)|stddev_samp(cmp_sex)|\n",
      "+-----------------+--------------------+\n",
      "|0.955001381078048| 0.20730111116897781|\n",
      "+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, stddev\n",
    "\n",
    "parsed.agg(avg('cmp_sex'), stddev('cmp_sex')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Filtering Data with `.filter` or `where`\n",
    "- `where` function is an alias for the `filter` function\n",
    "    - pass to the where function can include statements that would be valid inside a `WHERE` clause in **Spark SQL**. \n",
    "```Python\n",
    "df.filter(col('is_match') == False)\n",
    "\n",
    "df.where(\"is_match = true\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. `withColumn()` \n",
    "- Spark `withColumn()` is a transformation function of DataFrame that is used to \n",
    "    - create a new column/update existing columns\n",
    "    - manipulate the column values of all rows or selected rows on DataFrame.\n",
    "\n",
    "-  Syntax: `withColumn(colName : String, col : Column) -> DataFrame`\n",
    "    - `colName`: String – specify a new column you wanted to create. use an existing column to update the value.\n",
    "    - `col`: Column – column expression.\n",
    "\n",
    "```Python\n",
    "df = df.withColumn(col_name, df[ccol_name].cast(DoubleType()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spark SQL\n",
    "- to tell **Spark SQL execution** engine the name, say `\"linkage\"`, it should associate with the `parsed` DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.createOrReplaceTempView(\"linkage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|is_match|    CNT|\n",
      "+--------+-------+\n",
      "|   false|5728201|\n",
      "|    true|  20931|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT is_match, COUNT(*) AS CNT\n",
    "    FROM linkage\n",
    "    GROUP BY is_match\n",
    "    ORDER BY cnt DESC       \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fast Summary Statistics for DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "summary = parsed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+\n",
      "|summary|       cmp_fname_c1|      cmp_fname_c2|\n",
      "+-------+-------------------+------------------+\n",
      "|  count|            5748125|            103698|\n",
      "|   mean| 0.7129024704437266|0.9000176718903189|\n",
      "| stddev|0.38875835961628014|0.2713176105782334|\n",
      "|    min|                0.0|               0.0|\n",
      "|    max|                1.0|               1.0|\n",
      "+-------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.select('summary','cmp_fname_c1', 'cmp_fname_c2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "matches = parsed.where(\"is_match = true\") # need valid SQL inside a WHERE clause in Spark SQL. \n",
    "match_summary = matches.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "misses = parsed.filter(col('is_match') == False)\n",
    "miss_summary = misses.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pivoting and Reshaping DataFrames\n",
    "- `.toPandas()` Convert the DataFrames into pandas DataFrames\n",
    "- Reshape them, and convert \n",
    "- `spark.createDataFrame(pandas_df)` Convert pandas DataFrames back to Spark DataFrames\n",
    "\n",
    "Note: We can safely do this because of the small size of the summary, match_summary, and miss_summary DataFrames since pandas DataFrames reside in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_p = summary.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 12)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>cmp_fname_c1</th>\n",
       "      <th>cmp_fname_c2</th>\n",
       "      <th>cmp_lname_c1</th>\n",
       "      <th>cmp_lname_c2</th>\n",
       "      <th>cmp_sex</th>\n",
       "      <th>cmp_bd</th>\n",
       "      <th>cmp_bm</th>\n",
       "      <th>cmp_by</th>\n",
       "      <th>cmp_plz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>5749132</td>\n",
       "      <td>5749132</td>\n",
       "      <td>5748125</td>\n",
       "      <td>103698</td>\n",
       "      <td>5749132</td>\n",
       "      <td>2464</td>\n",
       "      <td>5749132</td>\n",
       "      <td>5748337</td>\n",
       "      <td>5748337</td>\n",
       "      <td>5748337</td>\n",
       "      <td>5736289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>33324.48559643438</td>\n",
       "      <td>66587.43558331935</td>\n",
       "      <td>0.7129024704437266</td>\n",
       "      <td>0.9000176718903189</td>\n",
       "      <td>0.3156278193080383</td>\n",
       "      <td>0.3184128315317443</td>\n",
       "      <td>0.955001381078048</td>\n",
       "      <td>0.22446526708507172</td>\n",
       "      <td>0.48885529849763504</td>\n",
       "      <td>0.2227485966810923</td>\n",
       "      <td>0.00552866147434343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>23659.859374488064</td>\n",
       "      <td>23620.487613269695</td>\n",
       "      <td>0.38875835961628014</td>\n",
       "      <td>0.2713176105782334</td>\n",
       "      <td>0.3342336339615828</td>\n",
       "      <td>0.36856706620066537</td>\n",
       "      <td>0.20730111116897781</td>\n",
       "      <td>0.41722972238462636</td>\n",
       "      <td>0.4998758236779031</td>\n",
       "      <td>0.4160909629831756</td>\n",
       "      <td>0.07414914925420046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>99980</td>\n",
       "      <td>100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                id_1                id_2         cmp_fname_c1  \\\n",
       "0   count             5749132             5749132              5748125   \n",
       "1    mean   33324.48559643438   66587.43558331935   0.7129024704437266   \n",
       "2  stddev  23659.859374488064  23620.487613269695  0.38875835961628014   \n",
       "3     min                   1                   6                  0.0   \n",
       "4     max               99980              100000                  1.0   \n",
       "\n",
       "         cmp_fname_c2        cmp_lname_c1         cmp_lname_c2  \\\n",
       "0              103698             5749132                 2464   \n",
       "1  0.9000176718903189  0.3156278193080383   0.3184128315317443   \n",
       "2  0.2713176105782334  0.3342336339615828  0.36856706620066537   \n",
       "3                 0.0                 0.0                  0.0   \n",
       "4                 1.0                 1.0                  1.0   \n",
       "\n",
       "               cmp_sex               cmp_bd               cmp_bm  \\\n",
       "0              5749132              5748337              5748337   \n",
       "1    0.955001381078048  0.22446526708507172  0.48885529849763504   \n",
       "2  0.20730111116897781  0.41722972238462636   0.4998758236779031   \n",
       "3                    0                    0                    0   \n",
       "4                    1                    1                    1   \n",
       "\n",
       "               cmp_by              cmp_plz  \n",
       "0             5748337              5736289  \n",
       "1  0.2227485966810923  0.00552866147434343  \n",
       "2  0.4160909629831756  0.07414914925420046  \n",
       "3                   0                    0  \n",
       "4                   1                    1  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_index to summary\n",
    "# transpose row -> column\n",
    "# reset index\n",
    "summary_p = summary_p.set_index('summary').transpose().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>summary</th>\n",
       "      <th>index</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>stddev</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_1</td>\n",
       "      <td>5749132</td>\n",
       "      <td>33324.48559643438</td>\n",
       "      <td>23659.859374488064</td>\n",
       "      <td>1</td>\n",
       "      <td>99980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_2</td>\n",
       "      <td>5749132</td>\n",
       "      <td>66587.43558331935</td>\n",
       "      <td>23620.487613269695</td>\n",
       "      <td>6</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cmp_fname_c1</td>\n",
       "      <td>5748125</td>\n",
       "      <td>0.7129024704437266</td>\n",
       "      <td>0.38875835961628014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cmp_fname_c2</td>\n",
       "      <td>103698</td>\n",
       "      <td>0.9000176718903189</td>\n",
       "      <td>0.2713176105782334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cmp_lname_c1</td>\n",
       "      <td>5749132</td>\n",
       "      <td>0.3156278193080383</td>\n",
       "      <td>0.3342336339615828</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "summary         index    count                mean               stddev  min  \\\n",
       "0                id_1  5749132   33324.48559643438   23659.859374488064    1   \n",
       "1                id_2  5749132   66587.43558331935   23620.487613269695    6   \n",
       "2        cmp_fname_c1  5748125  0.7129024704437266  0.38875835961628014  0.0   \n",
       "3        cmp_fname_c2   103698  0.9000176718903189   0.2713176105782334  0.0   \n",
       "4        cmp_lname_c1  5749132  0.3156278193080383   0.3342336339615828  0.0   \n",
       "\n",
       "summary     max  \n",
       "0         99980  \n",
       "1        100000  \n",
       "2           1.0  \n",
       "3           1.0  \n",
       "4           1.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>summary</th>\n",
       "      <th>field</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>stddev</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_1</td>\n",
       "      <td>5749132</td>\n",
       "      <td>33324.48559643438</td>\n",
       "      <td>23659.859374488064</td>\n",
       "      <td>1</td>\n",
       "      <td>99980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_2</td>\n",
       "      <td>5749132</td>\n",
       "      <td>66587.43558331935</td>\n",
       "      <td>23620.487613269695</td>\n",
       "      <td>6</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cmp_fname_c1</td>\n",
       "      <td>5748125</td>\n",
       "      <td>0.7129024704437266</td>\n",
       "      <td>0.38875835961628014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cmp_fname_c2</td>\n",
       "      <td>103698</td>\n",
       "      <td>0.9000176718903189</td>\n",
       "      <td>0.2713176105782334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cmp_lname_c1</td>\n",
       "      <td>5749132</td>\n",
       "      <td>0.3156278193080383</td>\n",
       "      <td>0.3342336339615828</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "summary         field    count                mean               stddev  min  \\\n",
       "0                id_1  5749132   33324.48559643438   23659.859374488064    1   \n",
       "1                id_2  5749132   66587.43558331935   23620.487613269695    6   \n",
       "2        cmp_fname_c1  5748125  0.7129024704437266  0.38875835961628014  0.0   \n",
       "3        cmp_fname_c2   103698  0.9000176718903189   0.2713176105782334  0.0   \n",
       "4        cmp_lname_c1  5749132  0.3156278193080383   0.3342336339615828  0.0   \n",
       "\n",
       "summary     max  \n",
       "0         99980  \n",
       "1        100000  \n",
       "2           1.0  \n",
       "3           1.0  \n",
       "4           1.0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_p = summary_p.rename(columns={'index':'field'}) # rename col \"index\" -> \"field\"\n",
    "summary_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_p = summary_p.rename_axis(None, axis=1) #remove the axis name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>stddev</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_1</td>\n",
       "      <td>5749132</td>\n",
       "      <td>33324.48559643438</td>\n",
       "      <td>23659.859374488064</td>\n",
       "      <td>1</td>\n",
       "      <td>99980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_2</td>\n",
       "      <td>5749132</td>\n",
       "      <td>66587.43558331935</td>\n",
       "      <td>23620.487613269695</td>\n",
       "      <td>6</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cmp_fname_c1</td>\n",
       "      <td>5748125</td>\n",
       "      <td>0.7129024704437266</td>\n",
       "      <td>0.38875835961628014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cmp_fname_c2</td>\n",
       "      <td>103698</td>\n",
       "      <td>0.9000176718903189</td>\n",
       "      <td>0.2713176105782334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cmp_lname_c1</td>\n",
       "      <td>5749132</td>\n",
       "      <td>0.3156278193080383</td>\n",
       "      <td>0.3342336339615828</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          field    count                mean               stddev  min     max\n",
       "0          id_1  5749132   33324.48559643438   23659.859374488064    1   99980\n",
       "1          id_2  5749132   66587.43558331935   23620.487613269695    6  100000\n",
       "2  cmp_fname_c1  5748125  0.7129024704437266  0.38875835961628014  0.0     1.0\n",
       "3  cmp_fname_c2   103698  0.9000176718903189   0.2713176105782334  0.0     1.0\n",
       "4  cmp_lname_c1  5749132  0.3156278193080383   0.3342336339615828  0.0     1.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 6)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quannguyen/repos/pyspark/venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/quannguyen/repos/pyspark/venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "summaryT = spark.createDataFrame(summary_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------------------+-------------------+---+------+\n",
      "|       field|  count|               mean|             stddev|min|   max|\n",
      "+------------+-------+-------------------+-------------------+---+------+\n",
      "|        id_1|5749132|  33324.48559643438| 23659.859374488064|  1| 99980|\n",
      "|        id_2|5749132|  66587.43558331935| 23620.487613269695|  6|100000|\n",
      "|cmp_fname_c1|5748125| 0.7129024704437266|0.38875835961628014|0.0|   1.0|\n",
      "|cmp_fname_c2| 103698| 0.9000176718903189| 0.2713176105782334|0.0|   1.0|\n",
      "|cmp_lname_c1|5749132| 0.3156278193080383| 0.3342336339615828|0.0|   1.0|\n",
      "|cmp_lname_c2|   2464| 0.3184128315317443|0.36856706620066537|0.0|   1.0|\n",
      "|     cmp_sex|5749132|  0.955001381078048|0.20730111116897781|  0|     1|\n",
      "|      cmp_bd|5748337|0.22446526708507172|0.41722972238462636|  0|     1|\n",
      "|      cmp_bm|5748337|0.48885529849763504| 0.4998758236779031|  0|     1|\n",
      "|      cmp_by|5748337| 0.2227485966810923| 0.4160909629831756|  0|     1|\n",
      "|     cmp_plz|5736289|0.00552866147434343|0.07414914925420046|  0|     1|\n",
      "+------------+-------+-------------------+-------------------+---+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "summaryT.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- field: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      " |-- mean: string (nullable = true)\n",
      " |-- stddev: string (nullable = true)\n",
      " |-- min: string (nullable = true)\n",
      " |-- max: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summaryT.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "for c in summaryT.columns:\n",
    "    if c == 'field':\n",
    "        continue\n",
    "    summaryT = summaryT.withColumn(c, summaryT[c].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- field: string (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- mean: double (nullable = true)\n",
      " |-- stddev: double (nullable = true)\n",
      " |-- min: double (nullable = true)\n",
      " |-- max: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summaryT.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def pivot_summary(desc):\n",
    "  # convert to pandas dataframe\n",
    "  desc_p = desc.toPandas()\n",
    "  # transpose\n",
    "  desc_p = desc_p.set_index('summary').transpose().reset_index()\n",
    "  desc_p = desc_p.rename(columns={'index':'field'})\n",
    "  desc_p = desc_p.rename_axis(None, axis=1)\n",
    "  # convert to Spark dataframe\n",
    "  descT = spark.createDataFrame(desc_p)\n",
    "  # convert metric columns to double from string\n",
    "  for c in descT.columns:\n",
    "    if c == 'field':\n",
    "      continue\n",
    "    else:\n",
    "      descT = descT.withColumn(c, descT[c].cast(DoubleType()))\n",
    "  return descT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quannguyen/repos/pyspark/venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/quannguyen/repos/pyspark/venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/quannguyen/repos/pyspark/venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/quannguyen/repos/pyspark/venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "match_summaryT = pivot_summary(match_summary)\n",
    "miss_summaryT = pivot_summary(miss_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+------------------+--------------------+---+-------+\n",
      "|       field|  count|              mean|              stddev|min|    max|\n",
      "+------------+-------+------------------+--------------------+---+-------+\n",
      "|        id_1|20931.0| 34575.72117911232|   21950.31285196913|5.0|99946.0|\n",
      "|        id_2|20931.0| 51259.95939037791|   24345.73345377519|6.0|99996.0|\n",
      "|cmp_fname_c1|20922.0|0.9973163859635038| 0.03650667584833679|0.0|    1.0|\n",
      "|cmp_fname_c2| 1333.0|0.9898900320318176| 0.08251973727615237|0.0|    1.0|\n",
      "|cmp_lname_c1|20931.0|0.9970152595958817|0.043118807533945126|0.0|    1.0|\n",
      "|cmp_lname_c2|  475.0| 0.969370167843852| 0.15345280740388917|0.0|    1.0|\n",
      "|     cmp_sex|20931.0| 0.987291577086618| 0.11201570591216435|0.0|    1.0|\n",
      "|      cmp_bd|20925.0|0.9970848267622461| 0.05391487659807981|0.0|    1.0|\n",
      "|      cmp_bm|20925.0|0.9979450418160095|0.045286127452170664|0.0|    1.0|\n",
      "|      cmp_by|20925.0|0.9961290322580645| 0.06209804856731055|0.0|    1.0|\n",
      "|     cmp_plz|20902.0|0.9584250310975027| 0.19962063345931919|0.0|    1.0|\n",
      "+------------+-------+------------------+--------------------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "match_summaryT.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------------------+--------------------+----+--------+\n",
      "|       field|    count|                mean|              stddev| min|     max|\n",
      "+------------+---------+--------------------+--------------------+----+--------+\n",
      "|        id_1|5728201.0|  33319.913548075565|  23665.760130330676| 1.0| 99980.0|\n",
      "|        id_2|5728201.0|   66643.44259218557|  23599.551728241313|30.0|100000.0|\n",
      "|cmp_fname_c1|5727203.0|  0.7118634802175091| 0.38908060096985553| 0.0|     1.0|\n",
      "|cmp_fname_c2| 102365.0|  0.8988473514090158| 0.27272090294010215| 0.0|     1.0|\n",
      "|cmp_lname_c1|5728201.0|  0.3131380113364304|  0.3322812130572686| 0.0|     1.0|\n",
      "|cmp_lname_c2|   1989.0| 0.16295544855122532|  0.1930236663528703| 0.0|     1.0|\n",
      "|     cmp_sex|5728201.0|  0.9548833918362851| 0.20755988859217375| 0.0|     1.0|\n",
      "|      cmp_bd|5727412.0|  0.2216425149788421|  0.4153518275558732| 0.0|     1.0|\n",
      "|      cmp_bm|5727412.0|   0.486995347986141|  0.4998308940493865| 0.0|     1.0|\n",
      "|      cmp_by|5727412.0|  0.2199230647280133|  0.4141943267142977| 0.0|     1.0|\n",
      "|     cmp_plz|5715387.0|0.002043781112285135|0.045161979893625206| 0.0|     1.0|\n",
      "+------------+---------+--------------------+--------------------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "miss_summaryT.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_summaryT.createOrReplaceTempView('match_desc')\n",
    "miss_summaryT.createOrReplaceTempView('miss_desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------------------+\n",
      "|       field|    total|               delta|\n",
      "+------------+---------+--------------------+\n",
      "|     cmp_plz|5736289.0|  0.9563812499852176|\n",
      "|cmp_lname_c2|   2464.0|  0.8064147192926266|\n",
      "|      cmp_by|5748337.0|  0.7762059675300512|\n",
      "|      cmp_bd|5748337.0|   0.775442311783404|\n",
      "|cmp_lname_c1|5749132.0|  0.6838772482594513|\n",
      "|      cmp_bm|5748337.0|  0.5109496938298685|\n",
      "|cmp_fname_c1|5748125.0|  0.2854529057459947|\n",
      "|cmp_fname_c2| 103698.0| 0.09104268062280174|\n",
      "|     cmp_sex|5749132.0|0.032408185250332844|\n",
      "+------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT a.field, a.count + b.count AS total, a.mean - b.mean AS delta\n",
    "    FROM match_desc a, miss_desc b \n",
    "    WHERE a.field = b.field AND a.field NOT IN (\"id_1\", \"id_2\")      \n",
    "    ORDER BY delta DESC, total DESC\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By this measure, `cmp_fname_c2` isn’t very useful because it’s missing a lot of the time, and the difference in the mean value for matches and nonmatches is relatively small — 0.09, for a score that ranges from 0 to 1. \n",
    "- The `cmp_sex` feature also isn’t particularly helpful because even though it’s available for any pair of records, the difference in means is just 0.03\n",
    "- Good features: `cmp_plz`, `cmp_by`, `cmp_bd`, `cmp_lname_c1`, and `cmp_bm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Scoring and Model Evaluation\n",
    "\n",
    "- `expr` function from `pyspark.sql.functions` parses an input expression string into the column that it represents. \n",
    "    - For our scoring function, we are going to sum up the value of five fields (`cmp_plz`, `cmp_by`, `cmp_bd`, `cmp_lname_c1`, and `cmp_bm`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cmp_lname_c1 + cmp_plz + cmp_by + cmp_bd + cmp_bm'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_features = [\"cmp_lname_c1\", \"cmp_plz\", \"cmp_by\", \"cmp_bd\", \"cmp_bm\"]\n",
    "\n",
    "sum_expression = \" + \".join(good_features)\n",
    "sum_expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| 3148| 8326|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|14055|94934|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|33948|34740|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|  946|71870|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|64880|71676|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "parsed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we’ll use 0 in place of the null value in our sum, for good_features col only\n",
    "scored = parsed.fillna(0, subset=good_features).\\\n",
    "                withColumn('score', expr(sum_expression)).\\\n",
    "                select('score', 'is_match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|score|is_match|\n",
      "+-----+--------+\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  4.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scored.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1.1. Cross Tabulation, or Crosstab\n",
    "- create a *contingency* table (which is sometimes called a **cross tabulation**, or **crosstab**) that counts the number of records whose scores fall above/below the threshold value crossed with the number of records in each of those categories that were/were not matches.\n",
    "\n",
    "- Since we don’t know what threshold value we’re going to use yet, let’s write a function that takes the scored DataFrame and the choice of threshold as parameters and computes the crosstabs using the DataFrame API:\n",
    "\n",
    "- Note: that we are including the `selectExpr` method of the *DataFrame API* to dynamically determine the value of the field named above based on the value of the `t` argument using Python’s f-string formatting syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def crossTabs(scored: DataFrame, t: DoubleType) -> DataFrame:\n",
    "  return  scored.selectExpr(f\"score >= {t} as above\", \"is_match\").\\\n",
    "          groupBy(\"above\").pivot(\"is_match\", (\"true\", \"false\")).\\\n",
    "          count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:=================>                                       (3 + 7) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+\n",
      "|above| true|  false|\n",
      "+-----+-----+-------+\n",
      "| true|20871|    637|\n",
      "|false|   60|5727564|\n",
      "+-----+-----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "crossTabs(scored, 4.0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+\n",
      "|above| true|  false|\n",
      "+-----+-----+-------+\n",
      "| true|20889|   6671|\n",
      "|false|   42|5721530|\n",
      "+-----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crossTabs(scored, 3.5).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f04691e337678ffb01a0e3ad1cff493c12469baaa3ddec5f329a3c762b8d2c56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
